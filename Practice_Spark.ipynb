{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ef3867-eb70-44a5-bf94-31d12621081a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# In Databricks, SparkSession is already available\n",
    "# For local testing, uncomment the following:\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Spark SQL Practice\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import col, to_timestamp, current_timestamp, expr, date_sub, date_format, sum as spark_sum, max as spark_max, row_number, window\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "print(\"Setup complete! SparkSession ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e01b8a-98d4-45fc-8aae-d22f98c4c698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create customers table\n",
    "# Schema: customer_id, region\n",
    "\n",
    "customers_data =[\n",
    "    (1, \"North\"),\n",
    "    (2, \"South\"),\n",
    "    (3, \"East\"),\n",
    "    (4, \"West\"),\n",
    "    (5, \"North\"),\n",
    "    (6, \"South\"),\n",
    "    (7, \"East\"),\n",
    "    (8, \"West\"),\n",
    "    (9, \"North\"),\n",
    "    (10, \"South\")\n",
    "]\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"region\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_customers = spark.createDataFrame(customers_data, customers_schema)\n",
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "\n",
    "print(\"Customers table created:\")\n",
    "df_customers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a788b3ac-a9c4-418b-8a47-d6c89b9889b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create orders table\n",
    "# Schema: order_id, customer_id, order_ts, amount\n",
    "# We'll create orders spanning the last 120 days to have data beyond the 90-day window\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Get current timestamp\n",
    "current_ts = datetime.now()\n",
    "\n",
    "# Generate orders over the last 120 days\n",
    "orders_data = []\n",
    "order_id = 1\n",
    "\n",
    "# Create orders for each customer across different dates\n",
    "for customer_id in range(1, 11):\n",
    "    # Create 2-4 orders per customer at different dates\n",
    "    num_orders = random.randint(2, 4)\n",
    "    for _ in range(num_orders):\n",
    "        # Random date within last 120 days\n",
    "        days_ago = random.randint(0, 120)\n",
    "        order_date = current_ts - timedelta(days=days_ago)\n",
    "        order_ts = order_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        amount = round(random.uniform(100, 2000), 2)\n",
    "        orders_data.append((order_id, customer_id, order_ts, amount))\n",
    "        order_id += 1\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_ts\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "df_orders = spark.createDataFrame(orders_data, orders_schema)\n",
    "# Convert order_ts to timestamp type\n",
    "df_orders = df_orders.withColumn(\"order_ts\", to_timestamp(col(\"order_ts\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_orders.createOrReplaceTempView(\"orders\")\n",
    "\n",
    "print(\"Orders table created:\")\n",
    "df_orders.orderBy(\"order_ts\").show(50, truncate=False)\n",
    "print(f\"\\nTotal orders: {df_orders.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64633706-0fab-440e-b6ff-ee34e59aa517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create payments table\n",
    "# Schema: payment_id, order_id, amount, paid_ts\n",
    "# Note: An order can have multiple payments (partial payments, refunds, etc.)\n",
    "# We need to identify the LATEST payment per order\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Get current timestamp\n",
    "current_ts = datetime.now()\n",
    "\n",
    "payments_data = []\n",
    "payment_id = 1\n",
    "\n",
    "# For each order, create 1-3 payments at different times\n",
    "for order_row in orders_data:\n",
    "    order_id = order_row[0]\n",
    "    order_date_str = order_row[2]\n",
    "    order_date = datetime.strptime(order_date_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    order_amount = order_row[3]\n",
    "    \n",
    "    # Create 1-3 payments per order\n",
    "    num_payments = random.randint(1, 3)\n",
    "    remaining_amount = order_amount\n",
    "    \n",
    "    for i in range(num_payments):\n",
    "        # Payment date is after order date, within 30 days\n",
    "        days_after_order = random.randint(0, 30)\n",
    "        payment_date = order_date + timedelta(days=days_after_order, hours=random.randint(0, 23))\n",
    "        paid_ts = payment_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        # Last payment gets remaining amount, others are partial\n",
    "        if i == num_payments - 1:\n",
    "            payment_amount = round(remaining_amount, 2)\n",
    "        else:\n",
    "            payment_amount = round(random.uniform(0.1, remaining_amount * 0.8), 2)\n",
    "            remaining_amount -= payment_amount\n",
    "        \n",
    "        payments_data.append((payment_id, order_id, payment_amount, paid_ts))\n",
    "        payment_id += 1\n",
    "\n",
    "payments_schema = StructType([\n",
    "    StructField(\"payment_id\", IntegerType(), True),\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"paid_ts\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_payments = spark.createDataFrame(payments_data, payments_schema)\n",
    "# Convert paid_ts to timestamp type\n",
    "df_payments = df_payments.withColumn(\"paid_ts\", to_timestamp(col(\"paid_ts\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "df_payments.createOrReplaceTempView(\"payments\")\n",
    "\n",
    "print(\"Payments table created:\")\n",
    "df_payments.orderBy(\"order_id\", \"paid_ts\").show(50, truncate=False)\n",
    "print(f\"\\nTotal payments: {df_payments.count()}\")\n",
    "\n",
    "# Show example: multiple payments for same order\n",
    "print(\"\\nExample: Multiple payments for order_id = 1:\")\n",
    "df_payments.filter(col(\"order_id\") == 1).orderBy(\"paid_ts\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de17d1cd-2b40-4c82-899d-82de362f7913",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Practice Question\n",
    "Task 1: Monthly Revenue by Region (Last 90 Days)\n",
    "Requirement: For the last 90 days, compute monthly revenue by region based on the latest payment per order.\n",
    "\n",
    "Key Points to Consider:\n",
    "\n",
    "Filter to last 90 days based on payment date (paid_ts)\n",
    "For each order, use only the latest payment (most recent paid_ts)\n",
    "Join with customers to get the region\n",
    "Group by month and region\n",
    "Sum the payment amounts\n",
    "Tables:\n",
    "\n",
    "customers(customer_id, region)\n",
    "orders(order_id, customer_id, order_ts, amount)\n",
    "payments(payment_id, order_id, amount, paid_ts)\n",
    "Expected Output Columns:\n",
    "\n",
    "month (e.g., \"2024-01\", \"2024-02\")\n",
    "region\n",
    "revenue (sum of latest payment amounts)\n",
    "Hints:\n",
    "\n",
    "Use window functions to identify the latest payment per order\n",
    "Consider using CTEs (Common Table Expressions) to break down the problem\n",
    "Remember to filter by date before applying window functions for better performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d51b1bb-f7bd-4fd6-bb67-d918d7f67883",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767932290872}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM payments;\n",
    "SELECT * FROM orders;\n",
    "SELECT * FROM customers;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "757c3479-1f9b-48e5-a69b-aa7f7a112d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "    SELECT \n",
    "        order_id,\n",
    "        COUNT(*) as payment_count,\n",
    "        MIN(paid_ts) as first_payment,\n",
    "        MAX(paid_ts) as latest_payment,\n",
    "        SUM(amount) as total_paid\n",
    "    FROM payments\n",
    "    GROUP BY order_id\n",
    "    HAVING payment_count > 1\n",
    "    ORDER BY payment_count DESC\n",
    "    LIMIT 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59219fe7-c27e-45e1-839e-2ebe57e83fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT \n",
    "        MIN(paid_ts) as earliest_payment,\n",
    "        MAX(paid_ts) as latest_payment,\n",
    "        COUNT(DISTINCT DATE_FORMAT(paid_ts, 'yyyy-MM')) as distinct_months,\n",
    "        COUNT(*) as total_payments\n",
    "    FROM payments\n",
    "    WHERE paid_ts >= current_timestamp() - INTERVAL 90 DAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08313693-8baf-4c10-acef-1f22a205a21a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check: Sample data exploration\n",
    "print(\"Sample customers:\")\n",
    "df_customers.show()\n",
    "\n",
    "print(\"\\nSample orders:\")\n",
    "df_orders.show(10)\n",
    "\n",
    "print(\"\\nSample payments:\")\n",
    "df_payments.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c4d1b8-07ad-45b9-b261-135b35a79e3f",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1767933928358}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "with ranking_cte as (\n",
    "SELECT payment_id, order_id, amount, paid_ts, dense_rank() OVER (PARTITION BY order_id ORDER BY paid_ts DESC) as Ranking FROM payments WHERE QUALIFY  Ranking = 1\n",
    "),\n",
    "filtering_cte AS (\n",
    "  SELECT r.payment_id, r.order_id, r.amount, r.paid_ts, r.Ranking, o.customer_id FROM ranking_cte r inner join orders o on r.order_id = o.order_id WHERE o.order_ts >= current_timestamp() - INTERVAL 90 DAYS\n",
    "),\n",
    "customer_join AS (\n",
    "SELECT f.customer_id, c.region, f.amount, DATE_TRUNC('month', f.paid_ts) as month FROM filtering_cte f inner join customers c on f.customer_id = c.customer_id\n",
    "),\n",
    "sum_amount_cte As (\n",
    "  SELECT date_format(month, 'yyyy-MM') as month_as , region, Round(sum(amount), 2) as revenue from customer_join GROUP By month_as, region\n",
    ")\n",
    "SELECT * FROM sum_amount_cte ORDER BY month_as DESC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93032916-31b9-4441-b9f2-52d57f704ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Check: Sample data exploration\n",
    "print(\"Sample customers:\")\n",
    "df_customers.show()\n",
    "\n",
    "print(\"\\nSample orders:\")\n",
    "df_orders.show(10)\n",
    "\n",
    "print(\"\\nSample payments:\")\n",
    "df_payments.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b99753-835a-45b7-b366-5b3b8ec594ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import dense_rank, desc, current_timestamp, expr\n",
    "\n",
    "window_spec = Window.partitionBy(\"order_id\").orderBy(col(\"paid_ts\").desc())\n",
    "df_ranking = df_payments.withColumn(\"Ranking\", dense_rank().over(window_spec))\n",
    "df_filtered = df_ranking.filter(col(\"Ranking\") == 1).drop(\"Ranking\")\n",
    "new_df = df_filtered.filter(expr(\"paid_ts >= current_timestamp() - INTERVAL 90 DAYS\"))\n",
    "\n",
    "p = df_filtered.alias(\"p\")     # payments\n",
    "o = df_orders.alias(\"o\")       # orders\n",
    "c = df_customers.alias(\"c\")    # customers\n",
    "\n",
    "df_join = (\n",
    "    p.join(o, on=\"order_id\", how=\"inner\")\n",
    "     .selectExpr(\n",
    "         \"o.customer_id\",\n",
    "         \"p.paid_ts\",\n",
    "         \"p.amount as payment_amount\"\n",
    "     )\n",
    "     .join(c, on=\"customer_id\", how=\"inner\")\n",
    "     .selectExpr(\n",
    "         \"customer_id\",\n",
    "         \"paid_ts\",\n",
    "         \"payment_amount\",\n",
    "         \"c.region\"\n",
    "     )\n",
    ")\n",
    "SELECT date_format(month, 'yyyy-MM') as month_as , region, Round(sum(amount), 2) as revenue from customer_join GROUP By month_as, region\n",
    "df_join.selectexpr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "060f50b8-7330-43a7-8aa2-574bd35d0aa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7513148593285476,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Practice_Spark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
